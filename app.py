import streamlit as st
from streamlit_drawable_canvas import st_canvas
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np
import cv2
import os
import streamlit.components.v1 as components
import re

# é é¢åŸºæœ¬è¨­å®š
st.set_page_config(page_title="AI æ•¸å­—è¾¨è­˜å°ˆå®¶", layout="centered")
st.title("ğŸ”¢ AI æ•¸å­—è¾¨è­˜å°ˆå®¶ (æ‰‹å¯« + èªéŸ³)")

# --- 1. æ ¸å¿ƒè½‰æ›å‡½æ•¸ï¼šå°‡ä¸­è‹±æ–‡å­—è½‰ç‚ºé˜¿æ‹‰ä¼¯æ•¸å­— ---
def text_to_digit(text):
    if not text: return ""
    mapping = {
        'é›¶': '0', 'ä¸€': '1', 'äºŒ': '2', 'å…©': '2', 'ä¸‰': '3', 'å››': '4', 'äº”': '5', 'å…­': '6', 'ä¸ƒ': '7', 'å…«': '8', 'ä¹': '9',
        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9'
    }
    # ç§»é™¤æ¨™é»ç¬¦è™Ÿä¸¦è½‰å°å¯«
    text = re.sub(r'[^\w\s]', '', text).lower()
    
    # æå–åŸæœ¬å°±æ˜¯æ•¸å­—çš„éƒ¨åˆ†
    digits = re.findall(r'\d', text)
    if digits: return "".join(digits)
    
    # é€å­—/é€è©è½‰æ›
    res = ""
    for char in text:
        if char in mapping: res += mapping[char]
    
    if not res: # å˜—è©¦è‹±æ–‡å–®è©æ‹†åˆ†
        for word in text.split():
            if word in mapping: res += mapping[word]
            
    return res

# --- 2. å¼·åŒ–ç‰ˆ CNN æ¨¡å‹è¼‰å…¥ ---
MODEL_PATH = 'mnist_model_final.h5'
@st.cache_resource
def get_model():
    if not os.path.exists(MODEL_PATH):
        mnist = tf.keras.datasets.mnist
        (x_train, y_train), _ = mnist.load_data()
        x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0
        model = models.Sequential([
            layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
            layers.MaxPooling2D((2, 2)),
            layers.Conv2D(64, (3, 3), activation='relu'),
            layers.Flatten(),
            layers.Dense(128, activation='relu'),
            layers.Dense(10, activation='softmax')
        ])
        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        model.fit(x_train, y_train, epochs=5, verbose=0)
        model.save(MODEL_PATH)
    return tf.keras.models.load_model(MODEL_PATH)

model = get_model()

# --- 3. ğŸ¤ èªéŸ³è¾¨è­˜å€å¡Š (å„ªåŒ–ç©©å®šæ€§) ---
st.subheader("ğŸ¤ èªéŸ³è¾¨è­˜æ•¸å­—")

# ä½¿ç”¨ Session State ç¢ºä¿çµæœæŒä¹…åŒ–
if "voice_result" not in st.session_state:
    st.session_state.voice_result = ""

# èªéŸ³é¡¯ç¤ºæ¡†
voice_display = st.text_input("èªéŸ³è­˜åˆ¥çµæœ (é˜¿æ‹‰ä¼¯æ•¸å­—)ï¼š", value=st.session_state.voice_result)

# JavaScript æ³¨å…¥ï¼šä½¿ç”¨æœ€æ–°çš„ Web Speech ç›£è½
speech_js = """
<script>
const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
recognition.lang = 'zh-TW';

function startListen() {
    const btn = document.getElementById("v_btn");
    btn.innerText = "æ­£åœ¨è†è½ä¸­...è«‹å¤§è²èªªå‡ºæ•¸å­—";
    btn.style.backgroundColor = "#ffa500";
    
    recognition.start();
    
    recognition.onresult = (event) => {
        const resultText = event.results[0][0].transcript;
        // é€é URL å‚³å€¼ä¸¦å¼·åˆ¶é‡æ–°è¼‰å…¥ï¼Œç¢ºä¿ Python èƒ½å¤ æŠ“åˆ°
        const url = new URL(window.location.href);
        url.searchParams.set('voice_input', resultText);
        window.parent.location.href = url.href; 
    };
    
    recognition.onerror = () => {
        btn.innerText = "è¾¨è­˜å¤±æ•—ï¼ŒæŒ‰æˆ‘é‡è©¦";
        btn.style.backgroundColor = "#ff4b4b";
    };
}
</script>
<button id="v_btn" onclick="startListen()" style="width:100%; padding:15px; background-color:#ff4b4b; color:white; border:none; border-radius:10px; cursor:pointer; font-weight:bold; font-size:1.1em; margin-bottom: 10px;">
    é»æ“Šé–‹å§‹èªéŸ³è¼¸å…¥ (æ”¯æ´ä¸€äºŒä¸‰ / One Two Three)
</button>
"""
components.html(speech_js, height=80)

# å¾ URL ç²å–ä¸¦æ›´æ–°
if st.query_params.get("voice_input"):
    raw_text = st.query_params.get("voice_input")
    converted = text_to_digit(raw_text)
    if converted != st.session_state.voice_result:
        st.session_state.voice_result = converted
        st.rerun() # å¼·åˆ¶åˆ·æ–° UI

if st.session_state.voice_result:
    st.info(f"ç³»çµ±åµæ¸¬åˆ°æ•¸å­—ï¼š{st.session_state.voice_result}")

# --- 4. âœï¸ æ‰‹å¯«è¾¨è­˜å€å¡Š ---
st.write("---")
st.subheader("âœï¸ æ‰‹å¯«è¾¨è­˜å€åŸŸ")
canvas_result = st_canvas(fill_color="rgba(255, 255, 255, 1)", stroke_width=18, stroke_color="#FFFFFF", background_color="#000000", height=300, width=600, drawing_mode="freedraw", key="canvas_v3")

if canvas_result.image_data is not None:
    img = cv2.cvtColor(canvas_result.image_data.astype('uint8'), cv2.COLOR_RGBA2GRAY)
    _, thresh = cv2.threshold(img, 50, 255, cv2.THRESH_BINARY)
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    boxes = sorted([cv2.boundingRect(c) for c in contours if cv2.boundingRect(c)[2] > 5], key=lambda x: x[0])

    if boxes:
        final_digits = []
        for x, y, w, h in boxes:
            roi = img[y:y+h, x:x+w]
            # ç½®ä¸­èˆ‡æ­¸ä¸€åŒ–
            pad = 20
            roi = cv2.copyMakeBorder(roi, pad, pad, pad, pad, cv2.BORDER_CONSTANT, value=0)
            roi = cv2.resize(roi, (28, 28))
            pred = model.predict(roi.reshape(1, 28, 28, 1).astype('float32')/255.0, verbose=0)
            final_digits.append(str(np.argmax(pred)))
        
        st.success(f"### æ‰‹å¯«è¾¨è­˜çµæœï¼š{''.join(final_digits)}")
