import streamlit as st
from streamlit_drawable_canvas import st_canvas
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np
import cv2
import os
import streamlit.components.v1 as components

# --- ç¶²é é…ç½® (Web Page Configuration) ---
# è¨­å®šç¶²é æ¨™é¡Œèˆ‡å¸ƒå±€ (Set page title and layout)
st.set_page_config(page_title="AI æ•¸å­—å…¨èƒ½è¾¨è­˜", layout="centered")
st.title("ğŸ”¢ AI æ•¸å­—å…¨èƒ½è¾¨è­˜ç³»çµ±")
st.markdown("### æ”¯æ´ã€Œæ‰‹å¯«ã€èˆ‡ã€ŒèªéŸ³ã€é›™æ¨¡è¾¨è­˜")

# --- 1. æ¨¡å‹èˆ‡é è™•ç†é‚è¼¯ (Model & Preprocessing Logic) ---
# å®šç¾©æ¨¡å‹å„²å­˜è·¯å¾‘ (Define the path to save the model)
MODEL_PATH = 'mnist_model_v2.h5'

@st.cache_resource
def get_model():
    """
    è¼‰å…¥æˆ–è¨“ç·´æ¨¡å‹ (Load or train the model)
    å¦‚æœæœ¬åœ°æ²’æœ‰æ¨¡å‹æª”ï¼Œå‰‡è‡ªå‹•ä½¿ç”¨ MNIST è³‡æ–™é›†é€²è¡Œè¨“ç·´ä¸¦å„²å­˜
    """
    if not os.path.exists(MODEL_PATH):
        # è¼‰å…¥ MNIST æ‰‹å¯«æ•¸å­—è³‡æ–™é›†
        mnist = tf.keras.datasets.mnist
        (x_train, y_train), _ = mnist.load_data()
        # è³‡æ–™é è™•ç†ï¼šèª¿æ•´ç¶­åº¦ä¸¦æ­£è¦åŒ– (Normalize data to 0-1)
        x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0
        
        # å»ºç«‹ CNN å·ç©ç¥ç¶“ç¶²è·¯æ¨¡å‹æ¶æ§‹
        model = models.Sequential([
            layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
            layers.MaxPooling2D((2, 2)),
            layers.Conv2D(64, (3, 3), activation='relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Flatten(),
            layers.Dense(128, activation='relu'),
            layers.Dense(10, activation='softmax') # è¼¸å‡º 0-9 çš„æ©Ÿç‡
        ])
        
        # ç·¨è­¯ä¸¦è¨“ç·´æ¨¡å‹
        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        model.fit(x_train, y_train, epochs=5, verbose=0)
        # å„²å­˜æ¨¡å‹è‡³æœ¬åœ°
        model.save(MODEL_PATH)
        
    return tf.keras.models.load_model(MODEL_PATH)

# åˆå§‹åŒ–æ¨¡å‹
model = get_model()

def pre_process_digit(roi):
    """
    å½±åƒé è™•ç† (Image Preprocessing)
    å°‡åˆ‡å‰²å‡ºçš„æ•¸å­—å½±åƒç¸®æ”¾ä¸¦æ”¾å…¥ 28x28 çš„ç•«å¸ƒä¸­å¿ƒï¼Œä»¥ç¬¦åˆ MNIST æ ¼å¼
    """
    if roi.size == 0: return None, None
    h, w = roi.shape
    # ä¿æŒæ¯”ä¾‹ç¸®æ”¾åˆ° 20 åƒç´ ä»¥å…§ (Resize while maintaining aspect ratio)
    new_h, new_w = (20, int(20*w/h)) if h > w else (int(20*h/w), 20)
    roi_resized = cv2.resize(roi, (max(1, new_w), max(1, new_h)))
    
    # å»ºç«‹ 28x28 çš„é»‘è‰²åº•åœ–
    final_img = np.zeros((28, 28), dtype=np.uint8)
    # å°‡ç¸®æ”¾å¾Œçš„æ•¸å­—è²¼åœ¨æ­£ä¸­å¤® (Place the resized image in the center)
    final_img[(28-new_h)//2:(28-new_h)//2+new_h, (28-new_w)//2:(28-new_w)//2+new_w] = roi_resized
    
    # å›å‚³æ¨¡å‹è¼¸å…¥æ ¼å¼ (1, 28, 28, 1) èˆ‡é è¦½åœ–
    return final_img.reshape(1, 28, 28, 1).astype('float32') / 255.0, final_img

# --- 2. èªéŸ³è¾¨è­˜åŠŸèƒ½ (Voice Recognition Function) ---
st.subheader("ğŸ¤ èªéŸ³è¾¨è­˜æ•¸å­—")
st.info("é»æ“Šä¸‹æ–¹æŒ‰éˆ•å¾Œï¼Œè«‹å°è‘—éº¥å…‹é¢¨èªªå‡ºæ•¸å­—ï¼ˆä¾‹å¦‚ï¼šä¸€äºŒä¸‰ æˆ– One Two Threeï¼‰")

# ä½¿ç”¨ HTML/JavaScript æ³¨å…¥ç€è¦½å™¨ Web Speech API
speech_script = """
<script>
// åˆå§‹åŒ–ç€è¦½å™¨èªéŸ³è¾¨è­˜å¼•æ“
const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
recognition.lang = 'zh-TW'; // è¨­å®šèªè¨€ç‚ºç¹é«”ä¸­æ–‡
recognition.interimResults = false; // åªåœ¨è¾¨è­˜çµæŸå¾Œå›å‚³çµæœ

function startListen() {
    recognition.start(); // é–‹å§‹ç›£è½éŒ„éŸ³
    recognition.onresult = (event) => {
        const text = event.results[0][0].transcript;
        // å°‡è¾¨è­˜æ–‡å­—å‚³å› Streamlit ç«¯çš„ voice_input çµ„ä»¶
        window.parent.postMessage({type: 'streamlit:set_widget_value', value: text, key: 'voice_input'}, '*');
        // å½ˆå‡ºæç¤ºè¦–çª—
        alert("ä½ èªªçš„æ˜¯ï¼š" + text);
    };
}
</script>
<button onclick="startListen()" style="padding: 10px 20px; background-color: #ff4b4b; color: white; border: none; border-radius: 5px; cursor: pointer;">
    é–‹å§‹èªéŸ³è¾¨è­˜
</button>
"""
# åµŒå…¥è‡ªå®šç¾© HTML/JS çµ„ä»¶
components.html(speech_script, height=70)

# é¡¯ç¤ºèªéŸ³è¾¨è­˜å›å‚³çš„çµæœ (Display speech-to-text result)
voice_text = st.text_input("èªéŸ³è­˜åˆ¥çµæœï¼š", key="voice_result_display")

# --- 3. æ‰‹å¯«è¾¨è­˜ä»‹é¢ (Handwriting Recognition Interface) ---
st.subheader("âœï¸ æ‰‹å¯«è¾¨è­˜å€åŸŸ")
# å»ºç«‹äº’å‹•å¼ç•«æ¿ (Create drawing canvas)
canvas_result = st_canvas(
    fill_color="rgba(255, 255, 255, 1)", 
    stroke_width=18, 
    stroke_color="#FFFFFF", 
    background_color="#000000", 
    height=300, 
    width=600, 
    drawing_mode="freedraw", 
    key="canvas"
)

# è¾¨è­˜é‚è¼¯ (Recognition Logic)
if canvas_result.image_data is not None:
    # 1. å–å¾—ç•«æ¿è³‡æ–™ä¸¦è½‰ç‚ºç°éš (Convert RGBA to Gray)
    img = cv2.cvtColor(canvas_result.image_data.astype('uint8'), cv2.COLOR_RGBA2GRAY)
    # 2. å½±åƒäºŒå€¼åŒ– (Binarization)
    _, thresh = cv2.threshold(img, 50, 255, cv2.THRESH_BINARY)
    # 3. å°‹æ‰¾å„å€‹æ•¸å­—çš„è¼ªå»“ (Find contours of digits)
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    # ä¾ç…§ X åº§æ¨™æ’åºï¼Œç¢ºä¿æ•¸å­—å¾å·¦åˆ°å³æ’åˆ—
    digit_boxes = sorted([cv2.boundingRect(cnt) for cnt in contours if cv2.boundingRect(cnt)[2] > 5], key=lambda b: b[0])

    if digit_boxes:
        results = []
        # é€ä¸€è™•ç†æ¯å€‹åµæ¸¬åˆ°çš„æ•¸å­—æ¡†
        for x, y, w, h in digit_boxes:
            # åˆ‡å‰²å‡ºå–®å€‹æ•¸å­—ä¸¦é€²è¡Œé è™•ç†
            processed_input, _ = pre_process_digit(img[y:y+h, x:x+w])
            if processed_input is not None:
                # ä½¿ç”¨ CNN æ¨¡å‹é€²è¡Œåˆ†é¡é æ¸¬
                prediction = model.predict(processed_input, verbose=0)
                # å–å¾—æ©Ÿç‡æœ€é«˜è€…ä½œç‚ºè¾¨è­˜çµæœ
                results.append(str(np.argmax(prediction)))
        
        # åœ¨ç¶²é é¡¯ç¤ºæœ€çµ‚æ‹¼æ¥çµæœ (Display concatenated results)
        st.success(f"## æ‰‹å¯«è¾¨è­˜çµæœï¼š{''.join(results)}")
