import streamlit as st
from streamlit_drawable_canvas import st_canvas
import tensorflow as tf
import numpy as np
import cv2
import os
import streamlit.components.v1 as components
import re

# 1. é é¢è¨­å®š
st.set_page_config(page_title="AI æ•¸å­—è¾¨è­˜å°ˆå®¶", layout="centered")
st.title("ğŸ”¢ AI æ•¸å­—è¾¨è­˜å°ˆå®¶")

# 2. è½‰æ›å‡½æ•¸
def text_to_digit(text):
    if not text: return ""
    mapping = {'é›¶':'0','ä¸€':'1','äºŒ':'2','å…©':'2','ä¸‰':'3','å››':'4','äº”':'5','å…­':'6','ä¸ƒ':'7','å…«':'8','ä¹':'9',
               'zero':'0','one':'1','two':'2','three':'3','four':'4','five':'5','six':'6','seven':'7','eight':'8','nine':'9'}
    text = text.lower()
    digits = re.findall(r'\d', text)
    if digits: return "".join(digits)
    res = "".join([mapping[char] for char in text if char in mapping])
    if not res:
        for word in text.split():
            if word in mapping: res += mapping[word]
    return res

# 3. è¼‰å…¥æ¨¡å‹ (ä¿æŒç©©å®š)
@st.cache_resource
def get_model():
    if not os.path.exists('mnist_model.h5'):
        # è‡ªå‹•å»ºç«‹ç°¡æ˜“æ¨¡å‹ç¢ºä¿ä¸å ±éŒ¯
        mnist = tf.keras.datasets.mnist
        (x_train, y_train), _ = mnist.load_data()
        x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0
        model = tf.keras.models.Sequential([
            tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(10, activation='softmax')
        ])
        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        model.fit(x_train, y_train, epochs=1, verbose=0)
        model.save('mnist_model.h5')
    return tf.keras.models.load_model('mnist_model.h5')

model = get_model()

# --- 4. ğŸ¤ èªéŸ³è¾¨è­˜çµ„ä»¶ (ä½¿ç”¨å›å‚³å€¼æ¨¡å¼) ---
st.subheader("ğŸ¤ èªéŸ³è¾¨è­˜æ•¸å­—")

# é€™è£¡æ˜¯é—œéµï¼šå»ºç«‹ä¸€å€‹èƒ½èˆ‡ Streamlit é€šè¨Šçš„è‡ªå®šç¾©çµ„ä»¶
def voice_component():
    # JavaScript ç¨‹å¼ç¢¼ï¼šé€é Streamlit.setComponentValue å‚³å€¼
    component_code = """
    <div id="root">
        <button id="v_btn" style="width:100%; padding:15px; background-color:#ff4b4b; color:white; border:none; border-radius:10px; cursor:pointer; font-weight:bold; font-size:1.1em;">
            é»æ“Šé–‹å§‹èªªè©±
        </button>
    </div>
    <script>
    function sendValue(value) {
        window.parent.postMessage({
            isStreamlitMessage: true,
            type: "streamlit:setComponentValue",
            value: value
        }, "*");
    }

    const btn = document.getElementById("v_btn");
    const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
    recognition.lang = 'zh-TW';

    btn.onclick = () => {
        btn.innerText = "æ­£åœ¨è†è½...è«‹èªªæ•¸å­—";
        btn.style.backgroundColor = "#ffa500";
        recognition.start();
    };

    recognition.onresult = (event) => {
        const text = event.results[0][0].transcript;
        sendValue(text); // å‚³å› Python
        btn.innerText = "è¾¨è­˜å®Œæˆï¼é»æ“Šå†æ¬¡èªªè©±";
        btn.style.backgroundColor = "#ff4b4b";
    };

    recognition.onerror = () => {
        btn.innerText = "å‡ºéŒ¯äº†ï¼Œè«‹é‡è©¦";
        btn.style.backgroundColor = "#ff4b4b";
    };
    </script>
    """
    return components.html(component_code, height=100)

# ç²å–ä¾†è‡ª JS çš„åŸå§‹æ–‡å­—
raw_voice_text = voice_component()

# åœ¨ Python ç«¯è™•ç†è½‰æ›
if "last_voice" not in st.session_state:
    st.session_state.last_voice = ""

# é€™è£¡å› ç‚º components.html çš„å›å‚³é™åˆ¶ï¼Œæˆ‘å€‘ç”¨å¦ä¸€ç¨®æ–¹å¼æŠ“å– URL (å‚™æ¡ˆ)
voice_param = st.query_params.get("voice_input", "")
if voice_param:
    st.session_state.last_voice = text_to_digit(voice_param)

# é¡¯ç¤ºçµæœæ¡†
st.text_input("èªéŸ³è­˜åˆ¥çµæœ (é˜¿æ‹‰ä¼¯æ•¸å­—)ï¼š", value=st.session_state.last_voice, key="v_box")

# --- 5. âœï¸ æ‰‹å¯«è¾¨è­˜ (ä¿æŒä¸è®Š) ---
st.write("---")
st.subheader("âœï¸ æ‰‹å¯«è¾¨è­˜å€åŸŸ")
canvas_result = st_canvas(fill_color="rgba(255, 255, 255, 1)", stroke_width=18, stroke_color="#FFFFFF", background_color="#000000", height=300, width=600, drawing_mode="freedraw", key="canvas_final")

if canvas_result.image_data is not None:
    img = cv2.cvtColor(canvas_result.image_data.astype('uint8'), cv2.COLOR_RGBA2GRAY)
    contours, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    boxes = sorted([cv2.boundingRect(c) for c in contours if cv2.boundingRect(c)[2] > 5], key=lambda x: x[0])
    
    if boxes:
        res = []
        for x,y,w,h in boxes:
            roi = cv2.resize(img[y:y+h, x:x+w], (28, 28))
            p = model.predict(roi.reshape(1,28,28,1)/255.0, verbose=0)
            res.append(str(np.argmax(p)))
        st.success(f"### æ‰‹å¯«è¾¨è­˜çµæœï¼š{''.join(res)}")
        const resultText = event.results[0][0].transcript;
        // é€é URL å‚³å€¼ä¸¦å¼·åˆ¶é‡æ–°è¼‰å…¥ï¼Œç¢ºä¿ Python èƒ½å¤ æŠ“åˆ°
        const url = new URL(window.location.href);
        url.searchParams.set('voice_input', resultText);
        window.parent.location.href = url.href; 
    };
    
    recognition.onerror = () => {
        btn.innerText = "è¾¨è­˜å¤±æ•—ï¼ŒæŒ‰æˆ‘é‡è©¦";
        btn.style.backgroundColor = "#ff4b4b";
    };
}
</script>
<button id="v_btn" onclick="startListen()" style="width:100%; padding:15px; background-color:#ff4b4b; color:white; border:none; border-radius:10px; cursor:pointer; font-weight:bold; font-size:1.1em; margin-bottom: 10px;">
    é»æ“Šé–‹å§‹èªéŸ³è¼¸å…¥ (æ”¯æ´ä¸€äºŒä¸‰ / One Two Three)
</button>
"""
components.html(speech_js, height=80)

# å¾ URL ç²å–ä¸¦æ›´æ–°
if st.query_params.get("voice_input"):
    raw_text = st.query_params.get("voice_input")
    converted = text_to_digit(raw_text)
    if converted != st.session_state.voice_result:
        st.session_state.voice_result = converted
        st.rerun() # å¼·åˆ¶åˆ·æ–° UI

if st.session_state.voice_result:
    st.info(f"ç³»çµ±åµæ¸¬åˆ°æ•¸å­—ï¼š{st.session_state.voice_result}")

# --- 4. âœï¸ æ‰‹å¯«è¾¨è­˜å€å¡Š ---
st.write("---")
st.subheader("âœï¸ æ‰‹å¯«è¾¨è­˜å€åŸŸ")
canvas_result = st_canvas(fill_color="rgba(255, 255, 255, 1)", stroke_width=18, stroke_color="#FFFFFF", background_color="#000000", height=300, width=600, drawing_mode="freedraw", key="canvas_v3")

if canvas_result.image_data is not None:
    img = cv2.cvtColor(canvas_result.image_data.astype('uint8'), cv2.COLOR_RGBA2GRAY)
    _, thresh = cv2.threshold(img, 50, 255, cv2.THRESH_BINARY)
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    boxes = sorted([cv2.boundingRect(c) for c in contours if cv2.boundingRect(c)[2] > 5], key=lambda x: x[0])

    if boxes:
        final_digits = []
        for x, y, w, h in boxes:
            roi = img[y:y+h, x:x+w]
            # ç½®ä¸­èˆ‡æ­¸ä¸€åŒ–
            pad = 20
            roi = cv2.copyMakeBorder(roi, pad, pad, pad, pad, cv2.BORDER_CONSTANT, value=0)
            roi = cv2.resize(roi, (28, 28))
            pred = model.predict(roi.reshape(1, 28, 28, 1).astype('float32')/255.0, verbose=0)
            final_digits.append(str(np.argmax(pred)))
        
        st.success(f"### æ‰‹å¯«è¾¨è­˜çµæœï¼š{''.join(final_digits)}")
