import streamlit as st
from streamlit_drawable_canvas import st_canvas
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np
import cv2
import os
import streamlit.components.v1 as components
import re

# è¨­å®šç¶²é è³‡è¨Š
st.set_page_config(page_title="AI æ•¸å­—å…¨èƒ½è¾¨è­˜", layout="centered")
st.title("ğŸ”¢ AI æ•¸å­—å…¨èƒ½è¾¨è­˜ç³»çµ±")
st.markdown("### æ”¯æ´ã€Œæ‰‹å¯«ã€èˆ‡ã€ŒèªéŸ³ã€é›™æ¨¡è¾¨è­˜ (è‡ªå‹•è½‰é˜¿æ‹‰ä¼¯æ•¸å­—)")

# --- 1. æ•¸å­—è½‰æ›å™¨å‡½æ•¸ ---
def text_to_digit(text):
    # å»ºç«‹è½‰æ›å­—å…¸
    mapping = {
        'é›¶': '0', 'ä¸€': '1', 'äºŒ': '2', 'å…©': '2', 'ä¸‰': '3', 'å››': '4', 'äº”': '5', 'å…­': '6', 'ä¸ƒ': '7', 'å…«': '8', 'ä¹': '9',
        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9'
    }
    # è½‰å°å¯«è™•ç†è‹±æ–‡
    text = text.lower()
    
    # 1. å…ˆè™•ç†ç›´æ¥æ˜¯æ•¸å­—çš„æƒ…æ³ (ä¾‹å¦‚ "123")
    digits_only = re.findall(r'\d', text)
    if digits_only:
        return "".join(digits_only)
    
    # 2. è™•ç†ä¸­è‹±æ–‡å–®å­— (ä¾‹å¦‚ "ä¸€äºŒä¸‰" æˆ– "one two")
    result = ""
    # ç°¡å–®çš„é€å­—/é€è©æ¯”å°
    for char in text:
        if char in mapping:
            result += mapping[char]
            
    # å¦‚æœé€å­—æ¯”å°æ²’çµæœï¼Œå˜—è©¦è‹±æ–‡å–®è©æ‹†åˆ†æ¯”å°
    if not result:
        words = text.split()
        for w in words:
            if w in mapping:
                result += mapping[w]
                
    return result if result else text

# --- 2. æ¨¡å‹è¼‰å…¥é‚è¼¯ (ä¿æŒä¸è®Š) ---
MODEL_PATH = 'mnist_model_v2.h5'
@st.cache_resource
def get_model():
    if not os.path.exists(MODEL_PATH):
        mnist = tf.keras.datasets.mnist
        (x_train, y_train), _ = mnist.load_data()
        x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0
        model = models.Sequential([
            layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
            layers.MaxPooling2D((2, 2)),
            layers.Conv2D(64, (3, 3), activation='relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Flatten(),
            layers.Dense(128, activation='relu'),
            layers.Dense(10, activation='softmax')
        ])
        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        model.fit(x_train, y_train, epochs=5, verbose=0)
        model.save(MODEL_PATH)
    return tf.keras.models.load_model(MODEL_PATH)

model = get_model()

# --- 3. èªéŸ³è¾¨è­˜åŠŸèƒ½èˆ‡ JavaScript æ³¨å…¥ ---
st.subheader("ğŸ¤ èªéŸ³è¾¨è­˜æ•¸å­—")

# é€é query_params ä¾†æ¥æ”¶ JavaScript å‚³å›çš„å€¼
if "voice_output" not in st.session_state:
    st.session_state.voice_output = ""

# JavaScript ä¿®æ­£ç‰ˆï¼šä½¿ç”¨ window.location.href æˆ– Streamlit åŸç”Ÿå›å‚³æ©Ÿåˆ¶
speech_script = """
<script>
const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
recognition.lang = 'zh-TW'; 
recognition.interimResults = false;

function startListen() {
    const btn = document.getElementById("record_btn");
    btn.innerText = "æ­£åœ¨è†è½ä¸­...";
    btn.style.backgroundColor = "#ffaa00";
    
    recognition.start();
    
    recognition.onresult = (event) => {
        const text = event.results[0][0].transcript;
        // é€é Streamlit çš„æ–¹å¼å°‡å€¼å‚³å›
        const link = document.createElement('a');
        link.href = `?voice_input=${encodeURIComponent(text)}`;
        link.click();
    };
    
    recognition.onerror = (event) => {
        alert("èªéŸ³è¾¨è­˜ç™¼ç”ŸéŒ¯èª¤: " + event.error);
        btn.innerText = "é–‹å§‹èªéŸ³è¾¨è­˜";
        btn.style.backgroundColor = "#ff4b4b";
    };
}
</script>
<button id="record_btn" onclick="startListen()" style="padding: 15px 30px; background-color: #ff4b4b; color: white; border: none; border-radius: 8px; cursor: pointer; font-size: 16px; width: 100%;">
    æŒ‰æˆ‘é–‹å§‹èªªè©±
</button>
"""
components.html(speech_script, height=100)

# ç²å–å¾ URL å‚³å›çš„èªéŸ³æ–‡å­—
query_params = st.query_params
raw_voice = query_params.get("voice_input", "")

if raw_voice:
    converted_digit = text_to_digit(raw_voice)
    st.success(f"è¾¨è­˜åˆ°çš„åŸå§‹èªéŸ³ï¼š{raw_voice}")
    st.text_input("èªéŸ³è­˜åˆ¥çµæœ (é˜¿æ‹‰ä¼¯æ•¸å­—)ï¼š", value=converted_digit, key="voice_display")
else:
    st.text_input("èªéŸ³è­˜åˆ¥çµæœ (é˜¿æ‹‰ä¼¯æ•¸å­—)ï¼š", value="", key="voice_display_empty")

# --- 4. æ‰‹å¯«è¾¨è­˜ä»‹é¢ (ä¿æŒä¹‹å‰å„ªåŒ–éçš„é‚è¼¯) ---
st.write("---")
st.subheader("âœï¸ æ‰‹å¯«è¾¨è­˜å€åŸŸ")
canvas_result = st_canvas(fill_color="rgba(255, 255, 255, 1)", stroke_width=18, stroke_color="#FFFFFF", background_color="#000000", height=300, width=600, drawing_mode="freedraw", key="canvas")

def pre_process_digit(roi):
    if roi.size == 0: return None
    h, w = roi.shape
    new_h, new_w = (20, int(20*w/h)) if h > w else (int(20*h/w), 20)
    roi_resized = cv2.resize(roi, (max(1, new_w), max(1, new_h)))
    final_img = np.zeros((28, 28), dtype=np.uint8)
    final_img[(28-new_h)//2:(28-new_h)//2+new_h, (28-new_w)//2:(28-new_w)//2+new_w] = roi_resized
    return final_img.reshape(1, 28, 28, 1).astype('float32') / 255.0

if canvas_result.image_data is not None:
    img = cv2.cvtColor(canvas_result.image_data.astype('uint8'), cv2.COLOR_RGBA2GRAY)
    _, thresh = cv2.threshold(img, 50, 255, cv2.THRESH_BINARY)
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    digit_boxes = sorted([cv2.boundingRect(cnt) for cnt in contours if cv2.boundingRect(cnt)[2] > 5], key=lambda b: b[0])

    if digit_boxes:
        results = []
        for x, y, w, h in digit_boxes:
            processed_input = pre_process_digit(img[y:y+h, x:x+w])
            if processed_input is not None:
                results.append(str(np.argmax(model.predict(processed_input, verbose=0))))
        st.success(f"## æ‰‹å¯«è¾¨è­˜çµæœï¼š{''.join(results)}")
